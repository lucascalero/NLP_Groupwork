{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "hw5_wordembeddings_starter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucascalero/NLP_HW5/blob/main/hw5_wordembeddings_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5H0wSJCskq3"
      },
      "source": [
        "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
        "\n",
        "Names: __Lucas Calero Forero, Rebecca McBrayer__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZN18gYtLI-e",
        "outputId": "5a61173c-c767-4c00-c540-7b659f54d294"
      },
      "source": [
        "import  urllib.request  # the lib that handles the url stuff\n",
        "\n",
        "un_words = set()\n",
        "words = []\n",
        "data =  urllib.request.urlopen(\"http://www.gutenberg.org/files/135/135-0.txt\") # it's a file like object and works just like a file\n",
        "for line in data: # files are iterabl\n",
        "    words.extend(line.split())\n",
        "    un_words.update(line.split())\n",
        "print(len(words))\n",
        "print(len(un_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "568710\n",
            "53605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kHx3d7AsksG"
      },
      "source": [
        "Step 1: Word2Vec paper questions\n",
        "---------------------------\n",
        "1. Describe how a CBOW word embedding is generated.\n",
        "\n",
        "A feedforward Neural Net without a hidden layer with a shared projection layer is used to classify the current word on every iteration based on its context (previous and future words). The output is a word embedding of size V.\n",
        "\n",
        "2. What is a CBOW word embedding and how is it different from a skip-gram word embedding?\n",
        "\n",
        "The biggest difference between skip-gram and CBOW is the fact that skip-gram uses the current word as the input into the classifier and predicts words within a range R of the word 1 < R < C where C is the maxiumum distance. This technique allows for weighted classification depending on the distance from w(i) and the current word.  \n",
        "\n",
        "3. What is the task that the authors use to evaluate the generated word embeddings?\n",
        "\n",
        "The authors use the Semantic-Syntactic Word Relationship test set to evaluate their generated word embeddings, a test set developed by them for this paper and for future research.\n",
        "\n",
        "4. What are PCA and t-SNE? Why are these important to the task of training and interpreting word embeddings?\n",
        "\n",
        "PCA and t-SNE are methods of dimensionality reduction, projecting high-dimensional word embeddings into lower dimensional space. This is important foor training and interpreting word embeddings since embeddings can become extremely high dimensional, and projecting them into a lower space will both speed up training time and make it easier for humans to visualize and therefore interpret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-74SfWAxsksL"
      },
      "source": [
        "Step 2: Train your own word embeddings\n",
        "--------------------------------\n",
        "\n",
        "(describe the Spooky Authors Dataset here)\n",
        "\n",
        "\n",
        "Describe what data set you have chosen to compare and contrast with the Spooky Authors Dataset. Make sure to describe where it comes from and it's general properties.\n",
        "\n",
        "(describe your dataset here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T03:27:00.340250Z",
          "start_time": "2020-10-24T03:26:59.570883Z"
        },
        "id": "c2xZ1kGYsksN"
      },
      "source": [
        "# import your libraries here\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGac-c9IsksO"
      },
      "source": [
        "### a) Train embeddings on GIVEN dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of4wMm5FPw7u",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d7d1fedf-e1cb-41f9-a7d6-d35badc00917"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "spooky = pd.read_csv('train.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9c3a7e96-74f6-41fc-863a-89d67edbeadb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9c3a7e96-74f6-41fc-863a-89d67edbeadb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train.csv to train (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T04:39:25.438770Z",
          "start_time": "2020-10-24T04:39:24.888507Z"
        },
        "id": "Jiq2Q1a3sksQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "341a055a-a628-4369-dae6-163352530b78"
      },
      "source": [
        "spooky_text = list(spooky['text'])\n",
        "\n",
        "exclude = set(string.punctuation)\n",
        "spooky_nopun = []\n",
        "for st in spooky_text:\n",
        "  st = ''.join(ch for ch in st if ch not in exclude)\n",
        "  spooky_nopun.append(st)\n",
        "\n",
        "spooky_text = [f\"<s> {sentence.lower()} </s>\" for sentence in spooky_nopun]\n",
        "print(spooky_text[:10])\n",
        "spooky_data = [sentence.split() for sentence in spooky_text]\n",
        "print(spooky_data[:10])\n",
        "# code to train your word embeddings\n",
        "\n",
        "# Read the file 'spooky-author-identification/train.csv' \n",
        "# and prepare the training data in the following format\n",
        "\n",
        "# data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "# \t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
        "# \t\t\t['yet', 'another', 'sentence'],\n",
        "# \t\t\t['one', 'more', 'sentence'],\n",
        "# \t\t\t['and', 'the', 'final', 'sentence']]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s> this process however afforded me no means of ascertaining the dimensions of my dungeon as i might make its circuit and return to the point whence i set out without being aware of the fact so perfectly uniform seemed the wall </s>', '<s> it never once occurred to me that the fumbling might be a mere mistake </s>', '<s> in his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction </s>', '<s> how lovely is spring as we looked from windsor terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair </s>', '<s> finding nothing else not even gold the superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk </s>', '<s> a youth passed in solitude my best years spent under your gentle and feminine fosterage has so refined the groundwork of my character that i cannot overcome an intense distaste to the usual brutality exercised on board ship i have never believed it to be necessary and when i heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew i felt myself peculiarly fortunate in being able to secure his services </s>', '<s> the astronomer perhaps at this point took refuge in the suggestion of non luminosity and here analogy was suddenly let fall </s>', '<s> the surcingle hung in ribands from my body </s>', '<s> i knew that you could not say to yourself stereotomy without being brought to think of atomies and thus of the theories of epicurus and since when we discussed this subject not very long ago i mentioned to you how singularly yet with how little notice the vague guesses of that noble greek had met with confirmation in the late nebular cosmogony i felt that you could not avoid casting your eyes upward to the great nebula in orion and i certainly expected that you would do so </s>', '<s> i confess that neither the structure of languages nor the code of governments nor the politics of various states possessed attractions for me </s>']\n",
            "[['<s>', 'this', 'process', 'however', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', 'as', 'i', 'might', 'make', 'its', 'circuit', 'and', 'return', 'to', 'the', 'point', 'whence', 'i', 'set', 'out', 'without', 'being', 'aware', 'of', 'the', 'fact', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall', '</s>'], ['<s>', 'it', 'never', 'once', 'occurred', 'to', 'me', 'that', 'the', 'fumbling', 'might', 'be', 'a', 'mere', 'mistake', '</s>'], ['<s>', 'in', 'his', 'left', 'hand', 'was', 'a', 'gold', 'snuff', 'box', 'from', 'which', 'as', 'he', 'capered', 'down', 'the', 'hill', 'cutting', 'all', 'manner', 'of', 'fantastic', 'steps', 'he', 'took', 'snuff', 'incessantly', 'with', 'an', 'air', 'of', 'the', 'greatest', 'possible', 'self', 'satisfaction', '</s>'], ['<s>', 'how', 'lovely', 'is', 'spring', 'as', 'we', 'looked', 'from', 'windsor', 'terrace', 'on', 'the', 'sixteen', 'fertile', 'counties', 'spread', 'beneath', 'speckled', 'by', 'happy', 'cottages', 'and', 'wealthier', 'towns', 'all', 'looked', 'as', 'in', 'former', 'years', 'heart', 'cheering', 'and', 'fair', '</s>'], ['<s>', 'finding', 'nothing', 'else', 'not', 'even', 'gold', 'the', 'superintendent', 'abandoned', 'his', 'attempts', 'but', 'a', 'perplexed', 'look', 'occasionally', 'steals', 'over', 'his', 'countenance', 'as', 'he', 'sits', 'thinking', 'at', 'his', 'desk', '</s>'], ['<s>', 'a', 'youth', 'passed', 'in', 'solitude', 'my', 'best', 'years', 'spent', 'under', 'your', 'gentle', 'and', 'feminine', 'fosterage', 'has', 'so', 'refined', 'the', 'groundwork', 'of', 'my', 'character', 'that', 'i', 'cannot', 'overcome', 'an', 'intense', 'distaste', 'to', 'the', 'usual', 'brutality', 'exercised', 'on', 'board', 'ship', 'i', 'have', 'never', 'believed', 'it', 'to', 'be', 'necessary', 'and', 'when', 'i', 'heard', 'of', 'a', 'mariner', 'equally', 'noted', 'for', 'his', 'kindliness', 'of', 'heart', 'and', 'the', 'respect', 'and', 'obedience', 'paid', 'to', 'him', 'by', 'his', 'crew', 'i', 'felt', 'myself', 'peculiarly', 'fortunate', 'in', 'being', 'able', 'to', 'secure', 'his', 'services', '</s>'], ['<s>', 'the', 'astronomer', 'perhaps', 'at', 'this', 'point', 'took', 'refuge', 'in', 'the', 'suggestion', 'of', 'non', 'luminosity', 'and', 'here', 'analogy', 'was', 'suddenly', 'let', 'fall', '</s>'], ['<s>', 'the', 'surcingle', 'hung', 'in', 'ribands', 'from', 'my', 'body', '</s>'], ['<s>', 'i', 'knew', 'that', 'you', 'could', 'not', 'say', 'to', 'yourself', 'stereotomy', 'without', 'being', 'brought', 'to', 'think', 'of', 'atomies', 'and', 'thus', 'of', 'the', 'theories', 'of', 'epicurus', 'and', 'since', 'when', 'we', 'discussed', 'this', 'subject', 'not', 'very', 'long', 'ago', 'i', 'mentioned', 'to', 'you', 'how', 'singularly', 'yet', 'with', 'how', 'little', 'notice', 'the', 'vague', 'guesses', 'of', 'that', 'noble', 'greek', 'had', 'met', 'with', 'confirmation', 'in', 'the', 'late', 'nebular', 'cosmogony', 'i', 'felt', 'that', 'you', 'could', 'not', 'avoid', 'casting', 'your', 'eyes', 'upward', 'to', 'the', 'great', 'nebula', 'in', 'orion', 'and', 'i', 'certainly', 'expected', 'that', 'you', 'would', 'do', 'so', '</s>'], ['<s>', 'i', 'confess', 'that', 'neither', 'the', 'structure', 'of', 'languages', 'nor', 'the', 'code', 'of', 'governments', 'nor', 'the', 'politics', 'of', 'various', 'states', 'possessed', 'attractions', 'for', 'me', '</s>']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T04:39:38.482701Z",
          "start_time": "2020-10-24T04:39:28.044970Z"
        },
        "id": "PYYVfwTssksa"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# The dimension of word embedding. \n",
        "# This variable will be used throughout the program\n",
        "# you may vary this as you desire\n",
        "EMBEDDINGS_SIZE = 200\n",
        "\n",
        "# Train the Word2Vec model from Gensim. \n",
        "# Below are the hyperparameters that are most relevant. \n",
        "# But feel free to explore other \n",
        "# options too:\n",
        "# sg = 1\n",
        "# window = 5\n",
        "# size = EMBEDDING_SIZE\n",
        "# min_count = 1\n",
        "\n",
        "\n",
        "model = Word2Vec(sentences=spooky_data, vector_size=EMBEDDINGS_SIZE, window=5, min_count=1, workers=4)\n",
        "\n",
        "model.save(\"word2vec.model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T04:39:43.448249Z",
          "start_time": "2020-10-24T04:39:43.444835Z"
        },
        "id": "zrHh8LcVskse"
      },
      "source": [
        "# if you save your Word2Vec as the variable model, this will \n",
        "# print out the vocabulary size\n",
        "print('Vocab size {}'.format(len(model.wv.vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T04:39:48.730304Z",
          "start_time": "2020-10-24T04:39:45.451960Z"
        },
        "id": "BBery6mTsksf"
      },
      "source": [
        "# You can save file in txt format, then load later if you wish.\n",
        "# model.wv.save_word2vec_format('embeddings.txt', binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrwA5GcLsksh"
      },
      "source": [
        "### b) Train embedding on YOUR dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSY9XPYMsktf"
      },
      "source": [
        "# then do a second data set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIQeLOassktg"
      },
      "source": [
        "What text-normalization and pre-processing did you do and why? __YOUR ANSWER HERE__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqUth5Q4skti"
      },
      "source": [
        "Step 3: Evaluate the differences between the word embeddings\n",
        "----------------------------\n",
        "\n",
        "(make sure to include graphs, figures, and paragraphs with full sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAFG9W7csktt"
      },
      "source": [
        "Cite your sources:\n",
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnORO71Csktu"
      },
      "source": [
        "Step 4: Feedforward Neural Language Model\n",
        "--------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TxcRBzPsktv"
      },
      "source": [
        "### a) First, encode  your text into integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-26T21:39:09.625031Z",
          "start_time": "2020-10-26T21:39:09.009109Z"
        },
        "id": "R6XALQsdsktw"
      },
      "source": [
        "# Importing utility functions from Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers import Embedding\n",
        "\n",
        "NGRAM = 3 # The size of the ngram language model you want to train\n",
        "\n",
        "# Initializing a Tokenizer\n",
        "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
        "# word to a unique index. (Note: Indexing starts from 0)\n",
        "# Example:\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(data)\n",
        "# encoded = tokenizer.texts_to_sequences(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T03:27:54.373208Z",
          "start_time": "2020-10-24T03:27:54.369835Z"
        },
        "id": "MFuFmD5nskty"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIPUwuiWskty"
      },
      "source": [
        "### b) Next, prepare your sequences from text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2PQ8zxxskt1"
      },
      "source": [
        "#### Fixed ngram based sequences (Used for Feedforward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkG7YzIHskt_"
      },
      "source": [
        "The training samples will be structured in the following format. \n",
        "Depending on which ngram model we choose, there will be (n-1) tokens \n",
        "in the input sequence (X) and we will need to predict the nth token (Y)\n",
        "\n",
        "            X,\t\t\t\t\t\t  y\n",
        "    this,    process               however\n",
        "    process, however               afforded\n",
        "    however, afforded\t           me"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T05:21:28.039381Z",
          "start_time": "2020-10-24T05:21:24.941885Z"
        },
        "id": "88m4QnyFskum"
      },
      "source": [
        "def generate_ngram_training_samples(ngram: list) -> list:\n",
        "    '''\n",
        "    Takes the encoded data (list of lists) and \n",
        "    generates the training samples out of it.\n",
        "    Parameters:\n",
        "    up to you, we've put in what we used\n",
        "    but you can add/remove as needed\n",
        "    return: \n",
        "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
        "    '''\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piNFl1xcskus"
      },
      "source": [
        "### c) Then, split the sequences into X and y and create a Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T05:21:31.213422Z",
          "start_time": "2020-10-24T05:21:31.061759Z"
        },
        "id": "tH0rRFcQskut"
      },
      "source": [
        "# Note here that the sequences were in the form: \n",
        "# sequence = [x1, x2, ... , x(n-1), y]\n",
        "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T05:21:34.675827Z",
          "start_time": "2020-10-24T05:21:33.315288Z"
        },
        "id": "dBH_YTJjskuu"
      },
      "source": [
        "def read_embeddings():\n",
        "    '''Loads and parses embeddings trained in earlier.\n",
        "    Parameters and return values are up to you.\n",
        "    '''\n",
        "    \n",
        "    # you may find generating the following two dicts useful:\n",
        "    # word to embedding : {'the':1, ...}\n",
        "    # index to embedding : {1:'the', ...} (inverse of word_2_embedding)\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T05:22:24.016237Z",
          "start_time": "2020-10-24T05:22:24.011220Z"
        },
        "id": "jTHAS91lsku8"
      },
      "source": [
        "def data_generator(X: list, y: list, num_sequences_per_batch: int) -> (list,list):\n",
        "    '''\n",
        "    Returns data generator to be used by feed_forward\n",
        "    https://wiki.python.org/moin/Generators\n",
        "    https://realpython.com/introduction-to-python-generators/\n",
        "    \n",
        "    Yields batches of embeddings and labels to go with them.\n",
        "    Use one hot vectors to encode the labels \n",
        "    (see the to_categorical function)\n",
        "    \n",
        "    '''\n",
        "    pass\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T05:22:55.470133Z",
          "start_time": "2020-10-24T05:22:55.398259Z"
        },
        "id": "WyoSHhZosku9"
      },
      "source": [
        "# Examples\n",
        "# initialize data_generator\n",
        "# num_sequences_per_batch = 128 # this is the batch size\n",
        "# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
        "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
        "\n",
        "# sample=next(train_generator) # this is how you get data out of generators\n",
        "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulEUUlVSskvG"
      },
      "source": [
        "### d) Train your models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T04:56:19.207252Z",
          "start_time": "2020-10-24T04:56:19.204894Z"
        },
        "id": "MKVd4PGiskvH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T03:56:50.919869Z",
          "start_time": "2020-10-24T03:56:50.779792Z"
        },
        "id": "N2C5YljMskvI"
      },
      "source": [
        "# code to train a feedforward neural language model \n",
        "# on a set of given word embeddings\n",
        "# make sure not to just copy + paste to train your two models\n",
        "\n",
        "# Define the model architecture using Keras Sequential API\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T04:01:50.757170Z",
          "start_time": "2020-10-24T03:56:53.620836Z"
        },
        "id": "BAt6IoFLskvM"
      },
      "source": [
        "# Start training the model\n",
        "model.fit(x=train_generator, \n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r91FFMy9skvN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfgS9hreskvO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYE1zMS-skvO"
      },
      "source": [
        "### e) Generate Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T04:13:54.425934Z",
          "start_time": "2020-10-24T04:13:54.418616Z"
        },
        "id": "FIoOC3G4skvP"
      },
      "source": [
        "# generate a sequence from the model\n",
        "def generate_seq(model: Sequential, \n",
        "                 tokenizer: Tokenizer, \n",
        "                 seed: list, \n",
        "                 n_words: int):\n",
        "    '''\n",
        "    Parameters:\n",
        "        model: your neural network\n",
        "        tokenizer: the keras preprocessing tokenizer\n",
        "        seed: [w1, w2, w(n-1)]\n",
        "        n_words: generate a sentence of length n_words\n",
        "    Returns: string sentence\n",
        "    '''\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-24T04:14:13.123529Z",
          "start_time": "2020-10-24T04:14:13.000264Z"
        },
        "id": "Mpy72JiaskvQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NoHqW14skvS"
      },
      "source": [
        "### f) Compare your generated sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mcoqEdrskvT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkoJKD--skvT"
      },
      "source": [
        "Sources Cited\n",
        "----------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS66pOaYskvr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}