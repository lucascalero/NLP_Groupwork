{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5H0wSJCskq3"
   },
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: __Lucas Calero Forero, Rebecca McBrayer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kHx3d7AsksG"
   },
   "source": [
    "Step 1: Word2Vec paper questions\n",
    "---------------------------\n",
    "1. Describe how a CBOW word embedding is generated.\n",
    "\n",
    "A feedforward Neural Net without a hidden layer with a shared projection layer is used to classify the current word on every iteration based on its context (previous and future words). The output is a word embedding of size V.\n",
    "\n",
    "2. What is a CBOW word embedding and how is it different from a skip-gram word embedding?\n",
    "\n",
    "The biggest difference between skip-gram and CBOW is the fact that skip-gram uses the current word as the input into the classifier and predicts words within a range R of the word 1 < R < C where C is the maxiumum distance. This technique allows for weighted classification depending on the distance from w(i) and the current word.  \n",
    "\n",
    "3. What is the task that the authors use to evaluate the generated word embeddings?\n",
    "\n",
    "The authors use the Semantic-Syntactic Word Relationship test set to evaluate their generated word embeddings, a test set developed by them for this paper and for future research.\n",
    "\n",
    "4. What are PCA and t-SNE? Why are these important to the task of training and interpreting word embeddings?\n",
    "\n",
    "PCA and t-SNE are methods of dimensionality reduction, projecting high-dimensional word embeddings into lower dimensional space. This is important foor training and interpreting word embeddings since embeddings can become extremely high dimensional, and projecting them into a lower space will both speed up training time and make it easier for humans to visualize and therefore interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-74SfWAxsksL"
   },
   "source": [
    "Step 2: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "The Spooky Authors dataset is an old competition dataset from Kaggle. It has example sentences from famous works of writing from three different authors. For this assignment, each data point is a single sentence written by one of the three gothic authors.\n",
    "\n",
    "Describe what data set you have chosen to compare and contrast with the Spooky Authors Dataset. Make sure to describe where it comes from and its general properties.\n",
    "\n",
    "We chose the text of Les Miserables as our second dataset. This comes from Project Gutenburg and has over 500,000 words in its translated form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    },
    "id": "c2xZ1kGYsksN"
   },
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "import string\n",
    "import urllib.request\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGac-c9IsksO"
   },
   "source": [
    "### a) Train embeddings on GIVEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "Of4wMm5FPw7u",
    "outputId": "d7d1fedf-e1cb-41f9-a7d6-d35badc00917"
   },
   "outputs": [],
   "source": [
    "# Read the file 'spooky-author-identification/train.csv' \n",
    "spooky = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:25.438770Z",
     "start_time": "2020-10-24T04:39:24.888507Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jiq2Q1a3sksQ",
    "outputId": "341a055a-a628-4369-dae6-163352530b78",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code to train your word embeddings\n",
    "spooky_text = list(spooky['text'])\n",
    "\n",
    "# Remove punctuation\n",
    "exclude = set(string.punctuation)\n",
    "spooky_nopun = []\n",
    "for st in spooky_text:\n",
    "  st = ''.join(ch for ch in st if ch not in exclude)\n",
    "  spooky_nopun.append(st)\n",
    "\n",
    "# Add start and ending tokens, and make all words lowercase\n",
    "spooky_text = [\"<s> {} </s>\".format(sentence.lower()) for sentence in spooky_nopun]\n",
    "spooky_data = [sentence.split() for sentence in spooky_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:38.482701Z",
     "start_time": "2020-10-24T04:39:28.044970Z"
    },
    "id": "PYYVfwTssksa"
   },
   "outputs": [],
   "source": [
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Train the Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "# sg = 1\n",
    "# window = 5\n",
    "# size = EMBEDDING_SIZE\n",
    "# min_count = 1\n",
    "\n",
    "model = Word2Vec(sentences=spooky_data, size=EMBEDDINGS_SIZE, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:43.448249Z",
     "start_time": "2020-10-24T04:39:43.444835Z"
    },
    "id": "zrHh8LcVskse"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 25413\n"
     ]
    }
   ],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "print('Vocab size {}'.format(len(model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:48.730304Z",
     "start_time": "2020-10-24T04:39:45.451960Z"
    },
    "id": "BBery6mTsksf"
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "model.wv.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrwA5GcLsksh"
   },
   "source": [
    "### b) Train embedding on YOUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the data from Project Gutenburg\n",
    "with urllib.request.urlopen(\"http://www.gutenberg.org/files/135/135-0.txt\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "text = content.decode('utf-8-sig')\n",
    "\n",
    "# Remove parts of the text that aren't the book itself, plus extraneous whitespace\n",
    "text = text[text.find(\"So long as there shall exist\"):text.find(\"Publisher of the Italian translation \")]\n",
    "text = ''.join(ch for ch in text if ch not in ['\\n','\\r'])\n",
    "\n",
    "# Tokenize\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "exclude = set(string.punctuation)\n",
    "text_nopun = []\n",
    "for st in sentences:\n",
    "  st = ''.join(ch for ch in st if ch not in exclude)\n",
    "  text_nopun.append(st)\n",
    "\n",
    "# Add start and ending tokens, and make all words lowercase\n",
    "text = [\"<s> {} </s>\".format(sentence.lower()) for sentence in text_nopun]\n",
    "lesmis_data = [sentence.split() for sentence in text]\n",
    "lesmis_data = list(filter(lambda a: len(a)>2, lesmis_data))  # Remove blank sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Train the Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "# sg = 1\n",
    "# window = 5\n",
    "# size = EMBEDDING_SIZE\n",
    "# min_count = 1\n",
    "\n",
    "lesmis_model = Word2Vec(sentences=lesmis_data, size=EMBEDDINGS_SIZE, window=5, min_count=1, workers=4)\n",
    "lesmis_model.save(\"lesmis_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 63355\n"
     ]
    }
   ],
   "source": [
    "lesmis_model.wv.save_word2vec_format('lesmis_embeddings.txt', binary=False)\n",
    "print('Vocab size {}'.format(len(lesmis_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIQeLOassktg"
   },
   "source": [
    "What text-normalization and pre-processing did you do and why? __We chose to do only very minimal text preprocessing, to keep our implementation simple. We removed punctuation and made all words lowercase, since those have no impact on the meaning of words (and therefore how they should be represented by an embedding. We added start and end tokens to make learning easier, but otherwise left the sentences alone. We did not lemmatize, as the different forms of words will have different meanings and therefore we decided to let them be embedded differently.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqUth5Q4skti"
   },
   "source": [
    "Step 3: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAFG9W7csktt"
   },
   "source": [
    "Cite your sources:\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnORO71Csktu"
   },
   "source": [
    "Step 4: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TxcRBzPsktv"
   },
   "source": [
    "### a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    },
    "id": "R6XALQsdsktw"
   },
   "outputs": [],
   "source": [
    "NGRAM = 3 # The size of the ngram language model you want to train\n",
    "\n",
    "flat_data = [word for sent in spooky_data for word in sent]\n",
    "spooky_vocab = list(set(flat_data))\n",
    "\n",
    "# Initializing a Tokenizer\n",
    "spooky_tokenizer = Tokenizer()\n",
    "spooky_tokenizer.fit_on_texts(flat_data)\n",
    "\n",
    "indexed_spooky_data = tokenizer.texts_to_sequences(spooky_data)\n",
    "indexed_spooky_vocab = tokenizer.texts_to_sequences(spooky_vocab)\n",
    "\n",
    "spooky_word_to_index = {word:index[0] for word, index in zip(list(spooky_vocab), indexed_spooky_vocab)}\n",
    "spooky_index_to_word = {index[0]:word for word, index in zip(list(spooky_vocab), indexed_spooky_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data = [word for sent in lesmis_data for word in sent]\n",
    "lesmis_vocab = list(set(flat_data))\n",
    "\n",
    "# Initializing a Tokenizer\n",
    "lesmis_tokenizer = Tokenizer()\n",
    "lesmis_tokenizer.fit_on_texts(flat_data)\n",
    "\n",
    "indexed_lesmis_data = tokenizer.texts_to_sequences(lesmis_data)\n",
    "indexed_lesmis_vocab = tokenizer.texts_to_sequences(lesmis_vocab)\n",
    "\n",
    "lesmis_word_to_index = {word:index[0] for word, index in zip(list(lesmis_vocab), indexed_lesmis_vocab)}\n",
    "lesmis_index_to_word = {index[0]:word for word, index in zip(list(lesmis_vocab), indexed_lesmis_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIPUwuiWskty"
   },
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2PQ8zxxskt1"
   },
   "source": [
    "#### Fixed ngram based sequences (Used for Feedforward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkG7YzIHskt_"
   },
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depending on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process               however\n",
    "    process, however               afforded\n",
    "    however, afforded\t           me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:28.039381Z",
     "start_time": "2020-10-24T05:21:24.941885Z"
    },
    "id": "88m4QnyFskum"
   },
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(content: list) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    ngrams = []\n",
    "    for sent in content:\n",
    "        for i in range(NGRAM-1, len(sent)):\n",
    "            ngram = [sent[j] for j in range(i-(NGRAM-1), i+1)]\n",
    "            ngrams.append(ngram)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piNFl1xcskus"
   },
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    },
    "id": "dBH_YTJjskuu"
   },
   "outputs": [],
   "source": [
    "def read_embeddings(path: string) -> (dict,dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters and return values are up to you.\n",
    "    '''\n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':1, ...}\n",
    "    # index to embedding : {1:'the', ...} (inverse of word_2_embedding)\n",
    "    \n",
    "    model = Word2Vec.load(path)\n",
    "    word_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "    for word in model.wv.vocab.keys():\n",
    "        index = word_to_index[word]\n",
    "        embedding = model.wv[word]\n",
    "        word_to_embedding[word] = embedding\n",
    "        index_to_embedding[index] = embedding\n",
    "    return word_to_embedding, index_to_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the x & y lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_ngrams = generate_ngram_training_samples(indexed_spooky_data)\n",
    "encoded_X = []\n",
    "encoded_Y = []\n",
    "for ngram in spooky_ngrams:\n",
    "    encoded_X.append(ngram[:NGRAM-1])\n",
    "    encoded_Y.append(ngram[-1])\n",
    "\n",
    "spooky_word_to_embedding, spooky_index_to_embedding = read_embeddings(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    },
    "id": "jTHAS91lsku8"
   },
   "outputs": [],
   "source": [
    "def data_generator(X: list, Y: list, num_sequences_per_batch: int) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    '''\n",
    "    num_sequences = 0\n",
    "    \n",
    "    while True:\n",
    "        next_X = X[num_sequences: num_sequences+num_sequences_per_batch]\n",
    "        next_X_vec = []\n",
    "        for X_sent in next_X:\n",
    "            X_sent_next = []\n",
    "            for ii in X_sent:\n",
    "                X_sent_next.extend(spooky_index_to_embedding[ii])\n",
    "            next_X_vec.append(X_sent_next)\n",
    "        \n",
    "        next_Y = Y[num_sequences: num_sequences+num_sequences_per_batch]\n",
    "        next_Y_cat = [tf.keras.utils.to_categorical(vec, num_classes=len(spooky_vocab)) for vec in next_Y]\n",
    "        next_Y_tensor = np.array([tf.keras.backend.constant(cat) for cat in next_Y_cat])\n",
    "        \n",
    "        yield tf.keras.backend.constant(next_X_vec),next_Y_tensor\n",
    "        num_sequences += num_sequences_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:55.470133Z",
     "start_time": "2020-10-24T05:22:55.398259Z"
    },
    "id": "WyoSHhZosku9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 400])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples\n",
    "# initialize data_generator\n",
    "sequences = indexed_spooky_data\n",
    "num_sequences_per_batch = 128 # this is the batch size\n",
    "steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "train_generator = data_generator(encoded_X, encoded_Y, num_sequences_per_batch)\n",
    "\n",
    "sample=next(train_generator) # this is how you get data out of generators\n",
    "sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulEUUlVSskvG"
   },
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    },
    "id": "N2C5YljMskvI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 1052)              421852    \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1052)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 128)               134784    \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 25413)             3278277   \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 25413)             0         \n",
      "=================================================================\n",
      "Total params: 3,834,913\n",
      "Trainable params: 3,834,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "\n",
    "# Define the model architecture using Keras Sequential API\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1052, input_dim=EMBEDDINGS_SIZE * (NGRAM-1)))\n",
    "model.add(tf.keras.layers.Activation('tanh'))\n",
    "model.add(tf.keras.layers.Dense(128, input_dim=EMBEDDINGS_SIZE * (NGRAM-1)))\n",
    "model.add(tf.keras.layers.Activation('tanh'))\n",
    "model.add(tf.keras.layers.Dense(25413))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "id": "BAt6IoFLskvM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 39s 255ms/step - loss: 4.3829e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23c3f405370>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training the model\n",
    "model.fit(x=train_generator,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYE1zMS-skvO"
   },
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_embedding = spooky_word_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    },
    "id": "FIoOC3G4skvP"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model: Sequential,\n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list, \n",
    "                 n_words: int):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network (trained)\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    # initialize prediction list\n",
    "    prediction_list = seed\n",
    "    \n",
    "    # Build the sentence\n",
    "    while len(prediction_list) < n_words:\n",
    "    \n",
    "        # get last (NGRAM-1) words of list\n",
    "        sample = prediction_list[-(NGRAM-1):]\n",
    "    \n",
    "        # Convert to embeddings, reshape properly\n",
    "        sample_embedded = [word_to_embedding[word] for word in sample]\n",
    "        sample_embedded = np.array(sample_embedded).reshape(-1, (NGRAM-1)*EMBEDDINGS_SIZE)\n",
    "    \n",
    "        # generate new word from these\n",
    "        predictions = model.predict(sample_embedded)\n",
    "        predicted_word_index = predictions.argmax()\n",
    "    \n",
    "        # convert new word index to word and add it to list\n",
    "        predicted_word = index_to_word[predicted_word_index]\n",
    "        prediction_list.append(predicted_word)\n",
    "    \n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:14:13.123529Z",
     "start_time": "2020-10-24T04:14:13.000264Z"
    },
    "id": "Mpy72JiaskvQ",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'the',\n",
       " 'cheated',\n",
       " 'welcomes',\n",
       " 'whirlwind',\n",
       " 'soundly',\n",
       " 'rumours',\n",
       " 'consumptive',\n",
       " 'porphyrogene',\n",
       " 'rumours',\n",
       " 'consumptive',\n",
       " 'porphyrogene',\n",
       " 'rumours',\n",
       " 'consumptive',\n",
       " 'porphyrogene',\n",
       " 'rumours',\n",
       " 'consumptive',\n",
       " 'porphyrogene',\n",
       " 'rumours',\n",
       " 'consumptive']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_seq(model, tokenizer, ['<s>', 'the'], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NoHqW14skvS"
   },
   "source": [
    "### f) Compare your generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mcoqEdrskvT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkoJKD--skvT"
   },
   "source": [
    "Sources Cited\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS66pOaYskvr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "hw5_wordembeddings_starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
