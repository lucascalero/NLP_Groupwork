{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5H0wSJCskq3"
   },
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: __Lucas Calero Forero, Rebecca McBrayer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kHx3d7AsksG"
   },
   "source": [
    "Step 1: Word2Vec paper questions\n",
    "---------------------------\n",
    "1. Describe how a CBOW word embedding is generated.\n",
    "\n",
    "A feedforward Neural Net without a hidden layer with a shared projection layer is used to classify the current word on every iteration based on its context (previous and future words). The output is a word embedding of size V.\n",
    "\n",
    "2. What is a CBOW word embedding and how is it different from a skip-gram word embedding?\n",
    "\n",
    "A CBOW word embedding is a grouping of vectorized representation of words. Its generated using a Continuous Bag of Words model that predicts and weights the current word based on its context. \n",
    "The biggest difference between the skip-gram model and CBOW model is the fact that skip-gram uses the current word as the input into the classifier to predict words within a range R of the word 1 < R < C where C is the maxiumum distance. This technique allows for weighted classification depending on the distance from w(i) and the current word. \n",
    "\n",
    "Their embeddings are quite similar in the end, one factor to consider between the two is the fact that skip-gram can be more accurate with less data.\n",
    "\n",
    "3. What is the task that the authors use to evaluate the generated word embeddings?\n",
    "\n",
    "The authors use the Semantic-Syntactic Word Relationship test set to evaluate their generated word embeddings, a test set developed by them for this paper and for future research.\n",
    "\n",
    "4. What are PCA and t-SNE? Why are these important to the task of training and interpreting word embeddings?\n",
    "\n",
    "PCA and t-SNE are methods of dimensionality reduction, projecting high-dimensional word embeddings into lower dimensional space. This is important foor training and interpreting word embeddings since embeddings can become extremely high dimensional, and projecting them into a lower space will both speed up training time and make it easier for humans to visualize and therefore interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-74SfWAxsksL"
   },
   "source": [
    "Step 2: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "The Spooky Authors dataset is an old competition dataset from Kaggle. It has example sentences from famous works of writing from three different authors. For this assignment, each data point is a single sentence written by one of the three gothic authors.\n",
    "\n",
    "Describe what data set you have chosen to compare and contrast with the Spooky Authors Dataset. Make sure to describe where it comes from and its general properties.\n",
    "\n",
    "We chose the text of Les Miserables as our second dataset. This comes from Project Gutenburg and has over 500,000 words in its translated form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    },
    "id": "c2xZ1kGYsksN"
   },
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "import string\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x8PdL5iDXGhh"
   },
   "outputs": [],
   "source": [
    "# Variable Definitions\n",
    "NGRAM = 3 # The size of the ngram language model you want to train\n",
    "\n",
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGac-c9IsksO"
   },
   "source": [
    "### a) Train embeddings on GIVEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Of4wMm5FPw7u"
   },
   "outputs": [],
   "source": [
    "# Read the file 'spooky-author-identification/train.csv' \n",
    "spooky = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:25.438770Z",
     "start_time": "2020-10-24T04:39:24.888507Z"
    },
    "id": "Jiq2Q1a3sksQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code to train your word embeddings\n",
    "spooky_text = list(spooky['text'])\n",
    "\n",
    "# Remove punctuation\n",
    "exclude = set(string.punctuation)\n",
    "spooky_nopun = []\n",
    "for st in spooky_text:\n",
    "    st = ''.join(ch for ch in st if ch not in exclude)\n",
    "    spooky_nopun.append(st)\n",
    "\n",
    "# Add start and ending tokens, and make all words lowercase\n",
    "spooky_text = [\"{} {} {}\".format(\"\".join([\"<s> \"] * (NGRAM - 1)),sentence.lower(),\"\".join([\"</s> \"] * (NGRAM-1))) for sentence in spooky_nopun]\n",
    "spooky_data = [sentence.split() for sentence in spooky_text]\n",
    "spooky_data = list(filter(lambda a: len(a)>(NGRAM-1)*2, spooky_data))  # Remove blank sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:38.482701Z",
     "start_time": "2020-10-24T04:39:28.044970Z"
    },
    "id": "PYYVfwTssksa"
   },
   "outputs": [],
   "source": [
    "# Train the Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "# sg = 1\n",
    "# window = 5\n",
    "# size = EMBEDDING_SIZE\n",
    "# min_count = 1\n",
    "\n",
    "spooky_model = Word2Vec(sentences=spooky_data, size=EMBEDDINGS_SIZE, window=7, min_count=2, workers=4, sg=1)\n",
    "spooky_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:43.448249Z",
     "start_time": "2020-10-24T04:39:43.444835Z"
    },
    "id": "zrHh8LcVskse",
    "outputId": "e5f7af78-bb99-4974-842d-12bb9e4d9f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 15935\n"
     ]
    }
   ],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "print('Vocab size {}'.format(len(spooky_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:48.730304Z",
     "start_time": "2020-10-24T04:39:45.451960Z"
    },
    "id": "BBery6mTsksf"
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "spooky_model.wv.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrwA5GcLsksh"
   },
   "source": [
    "### b) Train embedding on YOUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ldqTmd3-TqRK"
   },
   "outputs": [],
   "source": [
    "# Pull the data from Project Gutenburg\n",
    "with urllib.request.urlopen(\"http://www.gutenberg.org/files/135/135-0.txt\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "text = content.decode('utf-8-sig')\n",
    "\n",
    "# Remove parts of the text that aren't the book itself, plus extraneous whitespace\n",
    "text = text[text.find(\"So long as there shall exist\"):text.find(\"Publisher of the Italian translation \")]\n",
    "# text = ''.join(ch for ch in text if ch not in ['\\n','\\r'])\n",
    "text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "# Tokenize\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DHVRRsAyTqRK"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "exclude = set(string.punctuation)\n",
    "text_nopun = []\n",
    "for st in sentences:\n",
    "    st = ''.join(ch for ch in st if ch not in exclude)\n",
    "    text_nopun.append(st)\n",
    "\n",
    "# Add start and ending tokens, and make all words lowercase\n",
    "text = [\"{} {} {}\".format(\"\".join([\"<s> \"] * (NGRAM-1)),sentence.lower(),\"\".join([\"</s> \"] * (NGRAM-1))) for sentence in text_nopun]\n",
    "lesmis_data = [sentence.split() for sentence in text]\n",
    "lesmis_data = list(filter(lambda a: len(a)>(NGRAM-1)*2, lesmis_data))  # Remove blank sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7Ue_SAQ0TqRK"
   },
   "outputs": [],
   "source": [
    "# Train the Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "# sg = 1\n",
    "# window = 5\n",
    "# size = EMBEDDING_SIZE\n",
    "# min_count = 1\n",
    "\n",
    "lesmis_model = Word2Vec(sentences=lesmis_data, size=EMBEDDINGS_SIZE, window=7, min_count=2, workers=4, sg=1)\n",
    "lesmis_model.save(\"lesmis_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KNn8U8ZzTqRK",
    "outputId": "9f8f55c8-8a16-4da9-91b9-54993941454d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 15607\n"
     ]
    }
   ],
   "source": [
    "# lesmis_model.wv.save_word2vec_format('lesmis_embeddings.txt', binary=False)\n",
    "print('Vocab size {}'.format(len(lesmis_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIQeLOassktg"
   },
   "source": [
    "What text-normalization and pre-processing did you do and why? __We chose to do only very minimal text preprocessing, to keep our implementation simple. We removed punctuation and made all words lowercase, since those have no impact on the meaning of words (and therefore how they should be represented by an embedding. We added start and end tokens to make learning easier, but otherwise left the sentences alone. We did not lemmatize, as the different forms of words will have different meanings and therefore we decided to let them be embedded differently.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqUth5Q4skti"
   },
   "source": [
    "Step 3: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky_model.wv.similar_by_word('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesmis_model.wv.similar_by_word('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,200), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.wv.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(spooky_model, 'relatives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lesmis_model.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "most_similar_key, similarity = result[0]  # look at the first match\n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = TSNE(n_components=2, random_state=0).fit_transform(spooky_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_x = vis_data[:, 0]\n",
    "vis_y = vis_data[:, 1]\n",
    "plt.scatter(vis_x, vis_y, cmap=plt.cm.get_cmap(\"jet\", 10))\n",
    "plt.clim(-0.5, 9.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = TSNE(n_components=2, random_state=0).fit_transform(lesmis_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_x = vis_data[:, 0]\n",
    "vis_y = vis_data[:, 1]\n",
    "plt.scatter(vis_x, vis_y, cmap=plt.cm.get_cmap(\"jet\", 10))\n",
    "plt.clim(-0.5, 9.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAFG9W7csktt"
   },
   "source": [
    "Cite your sources:\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@aneesha/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229\n",
    "- https://indico.io/blog/visualizing-with-t-sne/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnORO71Csktu"
   },
   "source": [
    "Step 4: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TxcRBzPsktv"
   },
   "source": [
    "### a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word embedding dict to check if word exists in embedding (depending on if min_word>1)\n",
    "spooky_embedding_dict = spooky_model.wv\n",
    "lesmis_embedding_dict = lesmis_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    },
    "id": "R6XALQsdsktw"
   },
   "outputs": [],
   "source": [
    "flat_data = [word for sent in spooky_data for word in sent if word in spooky_embedding_dict]\n",
    "spooky_vocab = list(set(flat_data))\n",
    "\n",
    "# Initializing a Tokenizer\n",
    "spooky_tokenizer = Tokenizer()\n",
    "spooky_tokenizer.fit_on_texts(flat_data)\n",
    "\n",
    "indexed_spooky_data = spooky_tokenizer.texts_to_sequences(spooky_data)\n",
    "indexed_spooky_vocab = spooky_tokenizer.texts_to_sequences(spooky_vocab)\n",
    "\n",
    "spooky_word_to_index = {word:index[0] for word, index in zip(list(spooky_vocab), indexed_spooky_vocab)}\n",
    "spooky_index_to_word = {index[0]:word for word, index in zip(list(spooky_vocab), indexed_spooky_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RLGi17D_TqRL"
   },
   "outputs": [],
   "source": [
    "flat_data = [word for sent in lesmis_data for word in sent if word in lesmis_embedding_dict]\n",
    "lesmis_vocab = list(set(flat_data))\n",
    "\n",
    "# Initializing a Tokenizer\n",
    "lesmis_tokenizer = Tokenizer()\n",
    "lesmis_tokenizer.fit_on_texts(flat_data)\n",
    "\n",
    "indexed_lesmis_data = lesmis_tokenizer.texts_to_sequences(lesmis_data)\n",
    "indexed_lesmis_vocab = lesmis_tokenizer.texts_to_sequences(lesmis_vocab)\n",
    "\n",
    "lesmis_word_to_index = {word:index[0] for word, index in zip(list(lesmis_vocab), indexed_lesmis_vocab)}\n",
    "lesmis_index_to_word = {index[0]:word for word, index in zip(list(lesmis_vocab), indexed_lesmis_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIPUwuiWskty"
   },
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2PQ8zxxskt1"
   },
   "source": [
    "#### Fixed ngram based sequences (Used for Feedforward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkG7YzIHskt_"
   },
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depending on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process               however\n",
    "    process, however               afforded\n",
    "    however, afforded\t           me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:28.039381Z",
     "start_time": "2020-10-24T05:21:24.941885Z"
    },
    "id": "88m4QnyFskum"
   },
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(content: list) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    ngrams = []\n",
    "    for sent in content:\n",
    "        for i in range(NGRAM-1, len(sent)):\n",
    "            ngram = [sent[j] for j in range(i-(NGRAM-1), i+1)]\n",
    "            ngrams.append(ngram)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piNFl1xcskus"
   },
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    },
    "id": "dBH_YTJjskuu"
   },
   "outputs": [],
   "source": [
    "def read_embeddings(path: string, word_to_index: dict) -> (dict,dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters and return values are up to you.\n",
    "    '''\n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':1, ...}\n",
    "    # index to embedding : {1:'the', ...} (inverse of word_2_embedding)\n",
    "    \n",
    "    model = Word2Vec.load(path)\n",
    "    word_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "    for word in model.wv.vocab.keys():\n",
    "        index = word_to_index[word]\n",
    "        embedding = model.wv[word]\n",
    "        word_to_embedding[word] = embedding\n",
    "        index_to_embedding[index] = embedding\n",
    "    return word_to_embedding, index_to_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQtz1KlYTqRM"
   },
   "source": [
    "### Create the x & y lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Wfr63uJRTqRM"
   },
   "outputs": [],
   "source": [
    "def generate_encoded_input(source):\n",
    "    ngram_source = generate_ngram_training_samples(source)\n",
    "    encoded_X = []\n",
    "    encoded_Y = []\n",
    "    for ngram in ngram_source:\n",
    "        encoded_X.append(ngram[:NGRAM-1])\n",
    "        encoded_Y.append(ngram[-1])\n",
    "        \n",
    "    return encoded_X, encoded_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    },
    "id": "jTHAS91lsku8"
   },
   "outputs": [],
   "source": [
    "def data_generator(X: list, Y: list, vocab: list, num_sequences_per_batch: int, index_to_embedding: dict) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    '''\n",
    "    num_sequences = 0\n",
    "    \n",
    "    while True:\n",
    "        next_X = X[num_sequences: num_sequences+num_sequences_per_batch]\n",
    "        next_X_vec = []\n",
    "        for X_sent in next_X:\n",
    "            X_sent_next = []\n",
    "            for ii in X_sent:\n",
    "                X_sent_next.extend(index_to_embedding[ii])\n",
    "            next_X_vec.append(X_sent_next)\n",
    "        \n",
    "        next_Y = Y[num_sequences: num_sequences+num_sequences_per_batch]\n",
    "        next_Y_cat = [tf.keras.utils.to_categorical(vec, num_classes=len(vocab)) for vec in next_Y]\n",
    "        next_Y_tensor = np.array([tf.keras.backend.constant(cat) for cat in next_Y_cat])\n",
    "        \n",
    "        yield tf.keras.backend.constant(next_X_vec),next_Y_tensor\n",
    "        num_sequences += num_sequences_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:55.470133Z",
     "start_time": "2020-10-24T05:22:55.398259Z"
    },
    "id": "WyoSHhZosku9",
    "outputId": "25c9d474-392b-470d-cbf2-911f768736ef"
   },
   "outputs": [],
   "source": [
    "# initialize data_generator\n",
    "def get_training_generator(source, vocab, encoded_X, encoded_Y, index_to_embedding):\n",
    "    '''\n",
    "    Sets parameters and collects data to build the data generator object\n",
    "    '''\n",
    "    sequences = source\n",
    "    num_sequences_per_batch = 128 # this is the batch size\n",
    "    steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "    train_generator = data_generator(encoded_X, encoded_Y, vocab, num_sequences_per_batch, index_to_embedding)\n",
    "    \n",
    "    return train_generator, steps_per_epoch\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[1].shape # (batch_size, (n-1)*EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulEUUlVSskvG"
   },
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    },
    "id": "N2C5YljMskvI",
    "outputId": "11313ac6-47cd-4133-b078-2e0704c9fa95"
   },
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "\n",
    "def create_model(model_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(64, input_dim=EMBEDDINGS_SIZE * (NGRAM-1)))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dense(model_size))\n",
    "    model.add(tf.keras.layers.Activation('softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "id": "BAt6IoFLskvM",
    "outputId": "3584d44f-c467-46c4-d5be-5174fe588598",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "def train_model(model, train_generator, steps_per_epoch):\n",
    "    history = model.fit(x=train_generator,\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              epochs=3)\n",
    "    print(history.history['accuracy'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYE1zMS-skvO"
   },
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    },
    "id": "FIoOC3G4skvP"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model: Sequential,\n",
    "                 seed: list, \n",
    "                 n_words: int,\n",
    "                 word_to_embedding: dict,\n",
    "                 index_to_word: dict):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network (trained)\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "        word_to_embedding: dictionary mapping words to their embeddings\n",
    "        index_to_word: dictionary mapping indexes to the words they reference\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    # initialize prediction list\n",
    "    prediction_list = seed\n",
    "    \n",
    "    # Build the sentence\n",
    "    while len(prediction_list) < n_words:\n",
    "    \n",
    "        # get last (NGRAM-1) words of list\n",
    "        sample = prediction_list[-(NGRAM-1):]\n",
    "    \n",
    "        # Convert to embeddings, reshape properly\n",
    "        sample_embedded = [word_to_embedding[word] for word in sample]\n",
    "        sample_embedded = np.array(sample_embedded).reshape(-1, (NGRAM-1)*EMBEDDINGS_SIZE)\n",
    "    \n",
    "        # generate new word from these\n",
    "        probabilities = model.predict(sample_embedded)[0]\n",
    "        predicted_word_index = np.random.choice(np.arange(len(probabilities)), 1, p=probabilities)[0]\n",
    "        # predicted_word_index = probabilities.argmax()\n",
    "    \n",
    "        # convert new word index to word and add it to list\n",
    "        predicted_word = index_to_word[predicted_word_index]\n",
    "        prediction_list.append(predicted_word)\n",
    "        if predicted_word == \"</s>\":\n",
    "            break\n",
    "    \n",
    "    return \" \".join(prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 64)                25664     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15935)             1035775   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15935)             0         \n",
      "=================================================================\n",
      "Total params: 1,061,439\n",
      "Trainable params: 1,061,439\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "152/152 [==============================] - 29s 192ms/step - loss: 7.5522 - accuracy: 0.0611\n",
      "Epoch 2/3\n",
      "152/152 [==============================] - 28s 182ms/step - loss: 6.8755 - accuracy: 0.0866s - loss: 6 - ETA: 2s\n",
      "Epoch 3/3\n",
      "152/152 [==============================] - 28s 183ms/step - loss: 6.7520 - accuracy: 0.0960\n",
      "[0.06111225485801697, 0.08655427396297455, 0.0959601178765297]\n"
     ]
    }
   ],
   "source": [
    "# Spooky Model\n",
    "spooky_filepath = \"word2vec.model\"\n",
    "SPOOKY_MODEL_SIZE = 15935\n",
    "\n",
    "# Gather the input data\n",
    "spooky_word_to_embedding, spooky_index_to_embedding = read_embeddings(spooky_filepath, spooky_word_to_index)\n",
    "spooky_encoded_X, spooky_encoded_Y = generate_encoded_input(indexed_spooky_data)\n",
    "spooky_data_generator, steps_per_epoch = get_training_generator(indexed_spooky_data,\n",
    "                                                                spooky_vocab,\n",
    "                                                                spooky_encoded_X, \n",
    "                                                                spooky_encoded_Y, \n",
    "                                                                spooky_index_to_embedding)\n",
    "\n",
    "# Train the model\n",
    "spooky_model = create_model(SPOOKY_MODEL_SIZE)\n",
    "spooky_history = train_model(spooky_model, spooky_data_generator, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                25664     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15607)             1014455   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 15607)             0         \n",
      "=================================================================\n",
      "Total params: 1,040,119\n",
      "Trainable params: 1,040,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "229/229 [==============================] - 41s 178ms/step - loss: 7.1657 - accuracy: 0.0707\n",
      "Epoch 2/3\n",
      "229/229 [==============================] - 41s 178ms/step - loss: 6.7392 - accuracy: 0.0884s - loss: 6.7 - ETA: 1s - loss: 6.7450 \n",
      "Epoch 3/3\n",
      "229/229 [==============================] - 43s 189ms/step - loss: 6.4444 - accuracy: 0.0907s - loss:\n",
      "[0.07065365463495255, 0.08842794597148895, 0.0906795859336853]\n"
     ]
    }
   ],
   "source": [
    "# Les Mis Model\n",
    "lesmis_filepath = \"lesmis_word2vec.model\"\n",
    "LESMIS_MODEL_SIZE = 15607\n",
    "\n",
    "# gather the input data\n",
    "lesmis_word_to_embedding, lesmis_index_to_embedding = read_embeddings(lesmis_filepath, lesmis_word_to_index)\n",
    "lesmis_encoded_X, lesmis_encoded_Y = generate_encoded_input(indexed_lesmis_data)\n",
    "lesmis_data_generator, steps_per_epoch = get_training_generator(indexed_lesmis_data,\n",
    "                                                                lesmis_vocab,\n",
    "                                                                lesmis_encoded_X, \n",
    "                                                                lesmis_encoded_Y, \n",
    "                                                                lesmis_index_to_embedding)\n",
    "\n",
    "# Train the model\n",
    "lesmis_model = create_model(LESMIS_MODEL_SIZE)\n",
    "lesmis_history = train_model(lesmis_model, lesmis_data_generator, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:14:13.123529Z",
     "start_time": "2020-10-24T04:14:13.000264Z"
    },
    "id": "Mpy72JiaskvQ",
    "outputId": "d06cea57-b682-4993-f5c0-a45d63363e85",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and an precise negro it and the went characters being of your tink became bore realms size to\n",
      " as one march of energy is a zadok but with presence to the beheld of the loathed paralysed\n",
      " to poor removal nature river curiosity dark he i friends death expose reclaimed tumuli pinions scoundrelly perform side\n",
      " his abundantly hopes and oclock moslems it power and cases for to my politely repulsive christi connexion would\n",
      " hands to toward on the quietly of the passive reading did with burthen beheld a thoroughness inquisitive might\n",
      " the moment in been have previously and the general ladder of courtesy its mistaken and facial as expression\n",
      " with that not high its frame when man want and di forward and a complex although the had\n",
      " a revenge where uplands near to the seize in here scene me not received that he of a\n",
      " than the void notes gates in the my floated proceeded the may document his branches of preservation intercourse\n",
      " the knightly nothing he long strange up throat the colourless believed on a next depend of a manner\n",
      " so another for out of i little pang he to senses cabin the eight dusk panes door sensibility\n",
      " apparent were opportunity of the lovely me a nightingale west one the rejoinder excitement forever that all prefect\n",
      " he first were the nervous indescribably he did not what she had course disinclined of it wild of\n",
      " the than of guarantee might between the secure it but met with my nearly material below fuel find\n",
      " to to his moved dwelling in which it might open a unfamiliar of his to lost as upon\n",
      " so a detail which who chewed he expected i had some fallen up which up is a chain\n",
      " were his scarabæus phebe listened ferocity up having she only until the wore family the end and adapted\n",
      " that the severity under fly before fall in act and sufficient a end his a indicate and illness\n",
      " stood that their on his dark his casualties undisturbed know had anxiety sought forward a earnestly does in\n",
      " to would him may rodosto feeling doors eternal the hunt voice for the lion forgotten or lambent the\n"
     ]
    }
   ],
   "source": [
    "# Spooky Sentences!\n",
    "for _ in range(0,20):\n",
    "    print(generate_seq(spooky_model, [\"<s>\", \"<s>\"], 20, spooky_word_to_embedding, spooky_index_to_word)[7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is paper to fortune to to stepping one’s provençal life and darling round was ensued intervals the he\n",
      " would certain easily life” going was “there women this work of the marble according to strikes them police\n",
      " a earth is” pass see my monsieur wears what in said weather the plots of a torn as\n",
      " upon some moved which police were gazed sublime said a for jean time opened from into in the\n",
      " no bedroom who eyes will before m addressed the parish themselves the opposite penalty charming its attached his\n",
      " this respecting who stupor nothing there the woman of a mayor eyes good towards daybreak letting this “how\n",
      " done nor the on then to him but the director your yet me have been served think with\n",
      " would bound entering can her alms said the used is hour the all aurora bed of the his\n",
      " more accepted his booths god who again him that the wellbeloved silverware coteries the depths faults of them\n",
      " and was third the command was a or almost advanced been who since the pair which act would\n",
      " he had child step to library and further to gazed and me he prices a standing cried conduct\n",
      " almost than stood created the sulks twine that sadness best find and they livres more religion require wretch\n",
      " that crowned uttered a unfortunate club the threw and yourself humiliation bewilderment was squads prisoners responsible some a\n",
      " bit the fantine” probably knows himself flower they cords a forgotten a noise had monsieur no eaten a\n",
      " it he for his bells and madeleine had listened is is” the are strength than is it he\n",
      " he the ten flame we shall not path light fact of his which never dust open at whom\n",
      " a necessary that a wind going and himself bread bedroom said le armchair tomorrow make the mention to\n",
      " us says in the xviii of the d’angoulême prison hand himself to the inheritances me her about appointed\n",
      " thanked by the some place remain the element of under the reality he it own reappears one man\n",
      " permits miserable in his france when at few waste more i at he explained have have serve he\n"
     ]
    }
   ],
   "source": [
    "# Les Mis Sentences!\n",
    "for _ in range(20):\n",
    "    print(generate_seq(lesmis_model, [\"<s>\", \"<s>\"], 20, lesmis_word_to_embedding, lesmis_index_to_word)[7:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NoHqW14skvS"
   },
   "source": [
    "### f) Compare your generated sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mcoqEdrskvT"
   },
   "source": [
    "*Compare these with sentences that could be produced using Shannon’s method with the statistical n-gram language models that we implemented earlier in the semester. Do your neural language models produce n-grams that were not observed during training? (1 paragraph, you may support this answer with code as desired).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences generated with feedforward network are more unique and varied than those generated using Shannon's method. Both the sentences here and those generated through Shannon's method make some level of grammatical sense, but at least with this data Shannon's method would perform better. Since the statistical way of generating sentences cannot create n-grams that did not occur in the training text, the Shannon's method sentences would appear more regular and contain more meaning. The sentences generated here include n-grams that were not observed during training, and there is little semantic sense contained in the majority of these sentences. The sentences generated from the Spooky Authors dataset are particularly nonsensical, and the sentences generated from Les Mis are ever so slightly more coherent. We assume this is because we had significantly more training data from Les Mis, and that if we trained these models with more information we would get increasingly comprehensible results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkoJKD--skvT"
   },
   "source": [
    "Sources Cited\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bS66pOaYskvr"
   },
   "source": [
    "https://keras.io/guides/sequential_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "hw5_wordembeddings_starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
